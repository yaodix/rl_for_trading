'''
探索和利用，

'''

import numpy as np
import matplotlib.pyplot as plt

class BanditEnv:
  '''
    multi-armed bandit (MAB) 
  '''
  def __init__(self, p_dist, r_dist):

      self.p_dist = p_dist
      self.r_dist = r_dist
      self.n_bandits = len(p_dist)  # number of bandits

  def step(self, action):
      reward = 0
      done = True
      if np.random.uniform() < self.p_dist[action]:
          if not isinstance(self.r_dist[action], list):
              reward = self.r_dist[action]
          else:
              reward = np.random.normal(self.r_dist[action][0], self.r_dist[action][1])

      return 0, reward, done

  def reset(self):
      return 0
    
def pure_exploration(env, n_episodes=1000):
    '''
      纯探索策略
    '''  
    # 初始化Q值（每个臂的价值估计）和选择次数计数器
    Q = np.zeros((env.n_bandits), dtype=np.float64)
    N = np.zeros((env.n_bandits), dtype=np.int32)

    Qe = np.empty((n_episodes, env.n_bandits), dtype=np.float64)
    returns = np.empty(n_episodes, dtype=np.float64)
    actions = np.empty(n_episodes, dtype=np.int32)
    for e in range(n_episodes):
        action = np.random.randint(len(Q))
        _, reward, _, = env.step(action)
        N[action] += 1
        # moving average
        # 为什么计算Q的平均：Q值代表选择某个动作的期望奖励， 在统计学中，样本均值是总体期望的无偏估计
        Q[action] = Q[action] + (reward - Q[action])/N[action]  
        
        Qe[e] = Q
        returns[e] = reward
        actions[e] = action
    episode_mean_reward = np.cumsum(returns) / (np.arange(n_episodes) + 1)
    return returns, Qe, actions, episode_mean_reward
  

def pure_exploitation(env, n_episodes=1000):
    '''
    纯利用策略
    '''
    Q = np.zeros((env.n_bandits), dtype=np.float64)
    N = np.zeros((env.n_bandits), dtype=np.int32)

    Qe = np.empty((n_episodes, env.n_bandits), dtype=np.float64)
    returns = np.empty(n_episodes, dtype=np.float64)
    actions = np.empty(n_episodes, dtype=np.int32)
    for e in range(n_episodes):
        action = np.argmax(Q)
        _, reward, _,  = env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action])/N[action]
        
        Qe[e] = Q
        returns[e] = reward
        actions[e] = action
    episode_mean_reward = np.cumsum(returns) / (np.arange(len(returns)) + 1)
    return returns, Qe, actions, episode_mean_reward
  
def epsilon_greedy(env, epsilon=0.1, n_episodes=1000):
    Q = np.zeros((env.n_bandits), dtype=np.float64)
    N = np.zeros((env.n_bandits), dtype=np.int32)

    Qe = np.empty((n_episodes, env.n_bandits), dtype=np.float64)
    returns = np.empty(n_episodes, dtype=np.float64)
    actions = np.empty(n_episodes, dtype=np.int32)
    for e in range(n_episodes):
        if np.random.uniform() > epsilon:
            action = np.argmax(Q)
        else:
            action = np.random.randint(len(Q))

        _, reward, _= env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action])/N[action]
        
        Qe[e] = Q
        returns[e] = reward
        actions[e] = action
    episode_mean_reward = np.cumsum(returns) / (np.arange(len(returns)) + 1)
    return returns, Qe, actions, episode_mean_reward
  
def lin_dec_epsilon_greedy(env,
                           init_epsilon=1.0,
                           min_epsilon=0.01, 
                           decay_ratio=0.01, 
                           n_episodes=1000):
    '''
    早期的时候更多机会去探索，后期地时候更多机会去利用
    '''
    Q = np.zeros((env.n_bandits), dtype=np.float64)
    N = np.zeros((env.n_bandits), dtype=np.int32)

    Qe = np.empty((n_episodes, env.n_bandits), dtype=np.float64)
    returns = np.empty(n_episodes, dtype=np.float64)
    actions = np.empty(n_episodes, dtype=np.int32)
    for e in range(n_episodes):
        decay_episodes = e * decay_ratio
        epsilon = init_epsilon -  decay_episodes
        epsilon = max(epsilon, min_epsilon)
        if np.random.uniform() > epsilon:
            action = np.argmax(Q)
        else:
            action = np.random.randint(len(Q))

        _, reward, _= env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action])/N[action]
        
        Qe[e] = Q
        returns[e] = reward
        actions[e] = action
    episode_mean_reward = np.cumsum(returns) / (np.arange(len(returns)) + 1)
    return returns, Qe, actions,episode_mean_reward
  
def softmax_strategy(env, 
            min_temp = 0.01,
            n_episodes = 1000):
    '''
    基于Q值来随机选择不同的action，高Q值的action被选中概率越来越大。
    '''
    Q = np.zeros((env.n_bandits), dtype=np.float64)
    N = np.zeros((env.n_bandits), dtype=np.int32)

    Qe = np.empty((n_episodes, env.n_bandits), dtype=np.float64)
    returns = np.empty(n_episodes, dtype=np.float64)
    actions = np.empty(n_episodes, dtype=np.int32)
    temp = np.logspace(0, -4, n_episodes)  # 温度参数序列
    for e in range(n_episodes):
        temp_value = max(temp[e], min_temp)
        scaled_Q = Q / temp_value
        norm_Q = scaled_Q - np.max(scaled_Q)  # 数值稳定性处理
        exp_Q = np.exp(norm_Q)
        probs = exp_Q / np.sum(exp_Q)  # Softmax概率分布

        action = np.random.choice(np.arange(len(probs)), 
                                  size=1, 
                                  p=probs)[0]

        _, reward, _ = env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action])/N[action]
        Qe[e] = Q
        returns[e] = reward
        actions[e] = action
    episode_mean_reward = np.cumsum(returns) / (np.arange(len(returns)) + 1)
    return returns, Qe, actions,episode_mean_reward
  
def upper_confidence_bound(env, 
                           c=0.5, 
                           n_episodes=1000):
    Q = np.zeros((env.n_bandits), dtype=np.float64)
    N = np.zeros((env.n_bandits), dtype=np.int32)
    
    Qe = np.empty((n_episodes, env.n_bandits), dtype=np.float64)
    returns = np.empty(n_episodes, dtype=np.float64)
    actions = np.empty(n_episodes, dtype=np.int32)
    for e in range(n_episodes):
        action = e
        if e >= len(Q):
            U = c * np.sqrt(np.log(e)/N)
            action = np.argmax(Q + U)

        _, reward, _= env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action])/N[action]
        
        Qe[e] = Q
        returns[e] = reward
        actions[e] = action
    episode_mean_reward = np.cumsum(returns) / (np.arange(len(returns)) + 1)
    return returns, Qe, actions,episode_mean_reward
  
if __name__ == "__main__":
  FourArmBanditEnv = BanditEnv(p_dist=[0.05, 0.1, 0.3, 0.2], r_dist=[1, 1, 1, 1])
  returns, Qe, actions, mean_reward_pure_exp = pure_exploration(FourArmBanditEnv)
  print(Qe)
  plt.plot(mean_reward_pure_exp)
  plt.savefig("pure_exploration.png")
  
  _, Qe_pure_exploitation, _, mean_reward__pure_exploitation = pure_exploitation(FourArmBanditEnv)
  print(Qe_pure_exploitation)
  plt.plot(mean_reward__pure_exploitation)
  plt.savefig("pure_exploitation.png")
  
  returns, Qe_epsilon, actions, mean_reward_epsilon = epsilon_greedy(FourArmBanditEnv)
  print(Qe_epsilon)
  plt.plot(mean_reward_epsilon)
  plt.savefig("epsilon_greedy.png")
  
  returns, Qe_dec_epsilon, actions, mean_reward_dec_epsilon = lin_dec_epsilon_greedy(FourArmBanditEnv)
  print(Qe_dec_epsilon)
  plt.plot(mean_reward_dec_epsilon)
  plt.savefig("lin_dec_epsilon_greedy.png")
  
  _, Qe_softmax, _, mean_reward_softmax = softmax_strategy(FourArmBanditEnv)
  print(Qe_softmax)
  plt.plot(mean_reward_softmax)
  plt.savefig("softmax_strategy.png")
  
  _, Qe_UCB, _, mean_reward_UCB = upper_confidence_bound(FourArmBanditEnv)
  print(Qe_UCB)
  plt.plot(mean_reward_UCB)
  plt.savefig("upper_confidence_bound.png")