'''
è’™ç‰¹å¡æ´›ï¼ˆMonte Carlo, MCï¼‰æ–¹æ³• å¯ä»¥ ä»ç¯å¢ƒä¸­è·å¾—å³æ—¶å¥–åŠ±ï¼ˆæ¯ä¸€æ­¥çš„ ï¼‰ï¼Œä½†å®ƒ 
åªåœ¨ä¸€å¹•ç»“æŸä¹‹å æ‰ç”¨è¿™äº›å¥–åŠ±æ¥è®¡ç®—æ€»å›æŠ¥ ï¼Œå¹¶æ›´æ–°ä»·å€¼å‡½æ•°ã€‚

æ—¶åºå·®åˆ†ç®—æ³•å¯ä»¥åœ¨æ¯ä¸€æ­¥éƒ½æ›´æ–°ä»·å€¼å‡½æ•°ï¼Œè€Œä¸éœ€è¦ç­‰å¾…ä¸€å¹•ç»“æŸã€‚

è’™ç‰¹å¡æ´›çš„æ»‘åŠ¨å¹³å‡å…¬å¼  å˜æ¢ å­¦ä¹ ç‡å’ŒG(t)è¡¨è¾¾ï¼Œå½¢æˆæ›´æ–°å…¬å¼ çŠ¶æ€ä»·å€¼å‡½æ•°ï¼šğ‘‰(ğ‘†ğ‘¡) = ğ‘‰(ğ‘†ğ‘¡) + ğ›¼[ğ‘…ğ‘¡+1 + ğ›¾ğ‘‰(ğ‘†ğ‘¡+1) âˆ’ ğ‘‰(ğ‘†ğ‘¡)]
'''

from help import FrozenLake, print_policy, test_game
import numpy as np

def decay_schedule(init_value, min_value, decay_ratio, max_steps, log_start=-2, log_base=10):
    decay_steps = int(max_steps * decay_ratio)
    rem_steps = max_steps - decay_steps
    values = np.logspace(log_start, 0, decay_steps, base=log_base, endpoint=True)[::-1]
    values = (values - values.min()) / (values.max() - values.min())
    values = (init_value - min_value) * values + min_value
    values = np.pad(values, (0, rem_steps), 'edge')
    return values
  
alphas = decay_schedule(1, 0.0001, 0.8, 20000)

import matplotlib.pylab as plt

# plt.plot(alphas)
# plt.savefig('alphas.png')
def select_action(state, Q, epsilon):
    if np.random.random() > epsilon:
        return np.argmax(Q[state])
    else:
        return np.random.randint(len(Q[state]))
      

def sarsa(env, episodes=100, gamma=0.9, test_policy_freq=1000):
    nS, nA = 16, 4
    Q = np.zeros((nS, nA), dtype=np.float64)
    alphas = decay_schedule(0.5,0.01,0.5, episodes)
    epsilons = decay_schedule(1,0.01,0.8, episodes)
    
    for i in range(episodes): 
        state = env.reset()
        finished = False
        action = select_action(state, Q, epsilons[i])
        while not finished:
            next_state, reward, finished = env.step(action)
            next_action = select_action(next_state, Q, epsilons[i])
            target = reward + gamma * Q[next_state][next_action] * (not finished)
            error = target - Q[state][action]
            Q[state][action] = Q[state][action] + alphas[i] * error
            state, action = next_state, next_action

        pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]
        
        if i % test_policy_freq == 0:
                print("Test episode {} Reaches goal {:.2f}%. ".format
                (i, test_game(env, pi,)*100))

    return pi, Q
  
def q_learning(env,episodes=100,gamma=0.9,test_policy_freq=1000):
    nS, nA = 16,4
    Q = np.zeros((nS, nA), dtype=np.float64)
    alphas = decay_schedule(0.5,0.01,0.5, episodes)
    epsilons = decay_schedule(1,0.01,0.8, episodes)
    for i in range(episodes): 
        state = env.reset()
        finished = False
        while not finished:
            action = select_action(state, Q, epsilons[i])
            next_state, reward, finished = env.step(action)
            # å­¦ä¹ æ‰€ç”¨çš„Qå€¼å’Œsarsaä¸ä¸€æ ·
            target = reward + gamma * Q[next_state].max() * (not finished)
            # è®¡ç®—ç›®æ ‡å€¼ï¼ˆåŸºäºè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼‰
            error = target - Q[state][action]
            Q[state][action] = Q[state][action] + alphas[i] * error
            state = next_state


        pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]
        
        if i % test_policy_freq == 0:
                print("Test episode {} Reaches goal {:.2f}%. ".format
                (i, test_game(env, pi)*100))

    return pi, Q
  

if __name__ == '__main__':
  env = FrozenLake()
  # sarsaæ‰€åˆ©ç”¨çš„ä¿¡æ¯ä¸­å™ªå£°è¾ƒå°‘ï¼Œå­¦ä¹ æ¯”è’™ç‰¹å¡ç½—æ›´å¿«
  policy_sarsa, Q_sarsa = sarsa(env, episodes=20000) # Test episode 19000 Reaches goal 75.00%
  print_policy(policy_sarsa)
  
  policy_qlearning, Q_qlearning = q_learning(env, episodes=20000) # Test episode 19000 Reaches goal 75.00%
  print_policy(policy_qlearning)