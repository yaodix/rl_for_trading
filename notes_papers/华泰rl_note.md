### 概念名词

强化学习的**核心思想**是个体通过与环境的交互，从反馈信号中进行学习。

强化学习的结果是某种动作选择规则，称为策略（policy），表示从**状态到动作的映射**。

学习的最终目标是寻找一种最优策略，使得价值最大化

价值（value)与回报（return)的区别？

    在 t 时刻，未来可能有多种轨迹，回报 Gt具有不确定性，价值是随机变量 Gt的期望

回报是从当前时刻开始到结束的所有奖励的“折现”之和：
𝐺𝑡= 𝑅𝑡+1+ 𝛾𝑅𝑡+2+ 𝛾2𝑅𝑡+3+ ⋯ = ∑𝛾𝑘𝑅𝑡+𝑘+1

### 数学基础

强化学习的基础框架是马可夫决策过程，马尔可夫决策过程是强化学习的数学基础。

马尔可夫过程：描述了(环境)状态和状态转移矩阵

马儿可夫奖励过程：在马儿可夫过程的每个状态加入即时奖励(和折扣因子)，那么状态就有个价值这个概念。

马儿可夫决策过程：与马尔可夫奖励过程相比，马尔可夫决策过程在条件部分引入动作，从而赋予智能体主观
能动性

马尔可夫奖励过程中，状态转移以一定概率发生，智能体只能被动接受状态转移的结果，
无法主动选择进入某种状态。强化学习强调智能体与环境的交互，智能体理应能影响环境。
因此需要将智能体的动作引入马尔可夫奖励过程，这就是马尔可夫决策过程。也就是引入了强化学习中的策略。

  **动作价值函数**采用字母q，是状态s和动作a的函数，也称为Q函数（Q-function），代表
策略π、状态s下执行动作a的回报期望：
𝑞𝜋(𝑠, 𝑎) = 𝔼𝜋[𝐺𝑡|𝑆𝑡= 𝑠, 𝐴𝑡= 𝑎]

  Q函数是对智能体动作价值的评估，智能体可以根据不同动作的Q函数大小进行决策，如
选择Q函数最大的动作。

马尔可夫决策过程的最优策略π*(s)，是采取该状态s下最优动作价值函数值最高的动作

### 算法介绍

强化学习分为基于价值的方法和基于策略的方法

强化学习日频择时策略：介绍DQN上证指数日频择时策略构建方法，并进行参数敏感
性测试。
