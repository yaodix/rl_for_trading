### 动作

* **Action Space Composition** :
  * **Defines the possible actions**, typically "buy," "sell," and "hold." Actions should match real trading strategies, for example:

    * Buy/sell one share
    * Hold shares
  * **Granularity of Actions** : More options provide finer control but create a larger action space. For example:

    * Buy/sell 10 shares
    * Buy/sell 50 shares
    * Hold shares
* 动作设计
  * 3个动作

    * short，hold，long
  * 5个动作

    * 0 Hold, 1 Open Long, 2 Open Short, 3 Close Long, 4 Close Short

### 奖励

* **Reward Function** : Connects actions to their potential profit-related rewards. Three actions in focus:
  * **Buy** : Commonly set to zero; can be adjusted to alter buying frequency.
  * **Hold** : Typically starting at zero; adjustable to modify holding duration.
  * **Sell** : Utilizes either raw profit or log returns, balancing simplicity and mathematical properties.

### Reward Strategies:

* **Raw Profit** : Directly ties rewards to profit from a sell action.
* **Log Returns** : Provides additive and symmetric properties for better reward stability.

#### 使用对数收益

时间可加性意义：

* 累计奖励可以直接 **求和** ，无需乘法运算
* 符合强化学习中**总回报 = 即时奖励之和**的框架
* 便于TD误差计算和值函数估计

 对称性的意义：

* 奖励函数**正负尺度一致**
* 模型不会因数学不对称而**误判风险收益关系**
* 避免策略产生“怕亏但不怕赚”的扭曲
  * 模型会不自觉地学会“厌恶亏损”但“不追求利润”，导致策略保守、收益受限。
  * “怕亏但不怕赚”是指：简单收益率让模型误以为盈亏的数学期望对称，从而过度规避亏损、对盈利机会反应迟钝，最终导致策略保守、复利受损；对数收益率通过更准确的净值映射，让模型正确权衡风险与收益。

### Implementation Advice:

* Start with zero rewards for buy and hold.
* Employ raw profit or log returns for selling.
* Adjust rewards iteratively to shape agent behavior.
* Ensure rewards are balanced to avoid unintended agent actions (e.g., never selling).
* Maintain additive and symmetric properties for consistency.
